{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2be0d24-f0b2-4d4a-823c-b93295ed5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f77c017-e1ab-4f21-967f-5024362b8c36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (train_data, test_data) \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimdb_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      4\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "(train_data, test_data) = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3097420-a0ce-4603-a8df-b85c9ee94134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, b\"<br />\", b\" \")\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a0881-ca41-49e9-b5d9-dec422cc547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "texts = [str(ex.numpy()) for ex, _ in train_data.take(10000)]\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca63f87-db55-4b3c-a417-35be06f60406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map(text, label):\n",
    "    text = tf.py_function(lambda t: tokenizer.texts_to_sequences([t.numpy().decode(\"utf-8\")])[0],\n",
    "                          [text], Tout=tf.int64)\n",
    "    text = tf.keras.preprocessing.sequence.pad_sequences([text], maxlen=200)[0]\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14594b8b-db43-491d-ad53-9975af586134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(optimize=False):\n",
    "    ds = train_data.map(preprocess_text)\n",
    "    ds = ds.map(tokenize_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if optimize:\n",
    "        ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b2fe7-b4b8-4d03-830b-3170e3bdc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(10000, 16, input_length=200),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6123e-8e83-410f-a25f-18f90e13365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for tag, opt in [('Unoptimized', False), ('Optimized', True)]:\n",
    "    ds = make_dataset(optimize=opt)\n",
    "    model = build_model()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f\"\\nðŸ”¹ Training: {tag}\")\n",
    "    start = time.time()\n",
    "    model.fit(ds, epochs=1, steps_per_epoch=300, verbose=2)\n",
    "    end = time.time()\n",
    "    results[tag] = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841ca4d-f958-42ec-9aa8-db45d65deca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ•’ Training Time Comparison:\")\n",
    "for tag, sec in results.items():\n",
    "    print(f\"{tag}: {sec:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4791f-878f-492c-b62b-bad669ffaf22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
